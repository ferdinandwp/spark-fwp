{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.2.32:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fcc8bcd9f90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting up spark environment\n",
    "import os\n",
    "# os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-8-openjdk-amd64'\n",
    "# os.environ['SPARK_HOME'] = '/content/spark-2.3.1-bin-hadoop2.7'\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing out data reader\n",
    "from pyspark.sql.functions import to_timestamp, col, lit\n",
    "FILE_PATH = '/Users/ferdinand/Desktop/dataset/'\n",
    "dataset = os.path.join(FILE_PATH,'reported-crimes.csv')\n",
    "\n",
    "rc = spark.read.csv(dataset, header=True).withColumn('Date',to_timestamp(col('Date'),'MM-dd-yyyy hh-mm-ss aa') > lit('2018-11-12'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA FRAME OPERATIONS\n",
    "#df.take(n)\n",
    "#df.collect --> return all datasets\n",
    "#df.show(n)\n",
    "#df.limit(n)\n",
    "#df.head(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA SCHEMA\n",
    "# rc.types\n",
    "# df.printschema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ID', 'string'),\n",
       " ('Case Number', 'string'),\n",
       " ('Date', 'boolean'),\n",
       " ('Block', 'string'),\n",
       " ('IUCR', 'string'),\n",
       " ('Primary Type', 'string'),\n",
       " ('Description', 'string'),\n",
       " ('Location Description', 'string'),\n",
       " ('Arrest', 'string'),\n",
       " ('Domestic', 'string'),\n",
       " ('Beat', 'string'),\n",
       " ('District', 'string'),\n",
       " ('Ward', 'string'),\n",
       " ('Community Area', 'string'),\n",
       " ('FBI Code', 'string'),\n",
       " ('X Coordinate', 'string'),\n",
       " ('Y Coordinate', 'string'),\n",
       " ('Year', 'string'),\n",
       " ('Updated On', 'string'),\n",
       " ('Latitude', 'string'),\n",
       " ('Longitude', 'string'),\n",
       " ('Location', 'string')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc.columns\n",
    "rc.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, StringType, StructField, StructType, TimestampType, BooleanType, DoubleType, IntegerType\n",
    "# structType([\n",
    "#     StructField('ID', StringType, True),\n",
    "#     StructField('Case Number', StringType, True)\n",
    "#     StructField('Date',TimestampType, True)\n",
    "#      'Block',\n",
    "#      'IUCR',\n",
    "#      'Primary Type',\n",
    "#      'Description',\n",
    "#      'Location Description',\n",
    "#      'Arrest',\n",
    "#      'Domestic',\n",
    "#      'Beat',\n",
    "#      'District',\n",
    "#      'Ward',\n",
    "#      'Community Area',\n",
    "#      'FBI Code',\n",
    "#      'X Coordinate',\n",
    "#      'Y Coordinate',\n",
    "#      'Year',\n",
    "#      'Updated On',\n",
    "#      'Latitude',\n",
    "#      'Longitude',\n",
    "#      'Location'\n",
    "    \n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\n",
    "    ('ID',StringType()),\n",
    "    ('Case Number',StringType()),\n",
    "    ('Date',TimestampType()),\n",
    "    ('Block',StringType()),\n",
    "    ('IUCR',StringType()),\n",
    "    ('Primary Type',StringType()),\n",
    "    ('Description',StringType()),\n",
    "    ('Location Description',StringType()),\n",
    "    ('Arrest',StringType()),\n",
    "    ('Domestic',BooleanType()),\n",
    "    ('Beat',StringType()),\n",
    "    ('District',StringType()),\n",
    "    ('Ward',StringType()),\n",
    "    ('Community Area',StringType()),\n",
    "    ('FBI Code',StringType()),\n",
    "    ('X Coordinate',StringType()),\n",
    "    ('Y Coordinate',StringType()),\n",
    "    ('Year',IntegerType()),\n",
    "    ('Updated On',StringType()),\n",
    "    ('Latitude',DoubleType()),\n",
    "    ('Longitude',DoubleType()),\n",
    "    ('Location',StringType())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField (x[0],x[1],True) for x in labels])\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc = spark.read.csv(dataset, schema=schema)\n",
    "rc.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLUMN OPERATIONS\n",
    "#df.withColumn('DoubleColumn',2*df['current']) --> add new column\n",
    "#df.withColumnRenamed(current_name,new_name)\n",
    "#df.drop('column_name')\n",
    "#df.groupBy('column')\n",
    "#df.select('col_name').show(n)\n",
    "#df.select(rc.col_name).show(n)\n",
    "#df.select(col('col_name')).show(n)\n",
    "#df.select('col1','col2','col3').show(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add constant\n",
    "from pyspark.sql.functions import lit\n",
    "rc.withColumn('One',lit(1)).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|IUCR|\n",
      "+----+\n",
      "|1090|\n",
      "|1512|\n",
      "|1572|\n",
      "|2110|\n",
      "|0895|\n",
      "+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rc = rc.drop('IUCR')\n",
    "rc.select('IUCR').distinct().count()\n",
    "rc.select('IUCR').distinct().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROW MANIPULATION\n",
    "#df.filter(col('col_name') > 1)\n",
    "#df.select(col).distinct().show() --> unique row\n",
    "#df.orderBy(col('col_name'))--> sorting\n",
    "#df.union(df2)--> appending row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_day = spark.read.csv(dataset, header=True).withColumn('Date',to_timestamp(col('Date'),'MM-dd-yyyy hh:mm:ss aa') == lit('2018-11-12'))\n",
    "one_day.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rc.union(one_day).orderBy('Date',ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.groupBy('Primary Type').count().orderBy('count', ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.select('Arrest').distinct().show() #similar to .unique() in pd \n",
    "rc.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of crime that resulted in an arrest\n",
    "rc.filter(col('Arrest') == 'true').count()/rc.select('Arrest').count()\n",
    "rc.groupBy('Location Description').count().orderBy('count', ascending=False).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#built in function\n",
    "from pyspark.sql import functions\n",
    "# print(dir(functions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, upper, substring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(substring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.select(lower(col('Primary Type')), upper(col('Primary Type')), substring(col('Primary Type'),1,5)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min, max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.select(min(col('Date')), max(col('Date'))).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_add, date_sub\n",
    "help(date_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.select(date_sub(min(col('Date')),3), date_add(max(col('Date')),3)).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.select('Date').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|          Christmas|\n",
      "+-------------------+\n",
      "|2019-12-25 13:30:00|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DATE MANIPULATION\n",
    "from pyspark.sql.functions import to_date, to_timestamp, lit\n",
    "# 2019-12-25 13:30:00'\n",
    "df = spark.createDataFrame([('2019-12-25 13:30:00',)], ['Christmas'])\n",
    "df.show(1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+------------------------------------------------+\n",
      "|to_date(`Christmas`, 'yyyy-MM-dd HH:mm:ss')|to_timestamp(`Christmas`, 'yyyy-MM-dd HH:mm:ss')|\n",
      "+-------------------------------------------+------------------------------------------------+\n",
      "|                                 2019-12-25|                             2019-12-25 13:30:00|\n",
      "+-------------------------------------------+------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(to_date(col('Christmas'), 'yyyy-MM-dd HH:mm:ss'), to_timestamp(col('Christmas'), 'yyyy-MM-dd HH:mm:ss')).show(1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+------------------------------------------------+\n",
      "|to_date(`Christmas`, 'dd/MM/yyyy HH:mm:ss')|to_timestamp(`Christmas`, 'dd/MM/yyyy HH:mm:ss')|\n",
      "+-------------------------------------------+------------------------------------------------+\n",
      "|                                       null|                                            null|\n",
      "+-------------------------------------------+------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([('25/Dec/2019 13:30:00',)], ['Christmas'])\n",
    "df.select(to_date(col('Christmas'), 'dd/MM/yyyy HH:mm:ss'), to_timestamp(col('Christmas'), 'dd/MM/yyyy HH:mm:ss')).show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------------------+------------------+----+-------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+--------+---------+--------+\n",
      "|      ID|Case Number|                Date|             Block|IUCR|       Primary Type|         Description|Location Description|Arrest|Domestic|Beat|District|Ward|Community Area|FBI Code|X Coordinate|Y Coordinate|Year|          Updated On|Latitude|Longitude|Location|\n",
      "+--------+-----------+--------------------+------------------+----+-------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+--------+---------+--------+\n",
      "|11034701|   JA366925|01/01/2001 11:00:...|   016XX E 86TH PL|1153| DECEPTIVE PRACTICE|FINANCIAL IDENTIT...|           RESIDENCE| false|   false|0412|     004|   8|            45|      11|        null|        null|2001|08/05/2017 03:50:...|    null|     null|    null|\n",
      "|11227287|   JB147188|10/08/2017 03:00:...|092XX S RACINE AVE|0281|CRIM SEXUAL ASSAULT|      NON-AGGRAVATED|           RESIDENCE| false|   false|2222|     022|  21|            73|      02|        null|        null|2017|02/11/2018 03:57:...|    null|     null|    null|\n",
      "+--------+-----------+--------------------+------------------+----+-------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+--------+---------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nrc = spark.read.csv(dataset, header='True')\n",
    "nrc.show(2, truncate=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function lpad in module pyspark.sql.functions:\n",
      "\n",
      "lpad(col, len, pad)\n",
      "    Left-pad the string column to width `len` with `pad`.\n",
      "    \n",
      "    >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "    >>> df.select(lpad(df.s, 6, '#').alias('s')).collect()\n",
      "    [Row(s='##abcd')]\n",
      "    \n",
      "    .. versionadded:: 1.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Data manipulation\n",
    "from pyspark.sql.functions import lpad\n",
    "help(lpad)\n",
    "#JOIN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.join(ps, rc.col == ps.col, 'left-outer').show(5, truncate=False)\n",
    "nc = rc.filter( (col('Primary Type') == 'NON - CRIMINAL') | (col('Primary Type') == 'NON-CRIMINAL') | ((col('Primary Type') == 'NON-CRIMINAL(SUB)')) )\n",
    "nc.groupBy(col('Description')).count(orderBy('count', ascending=False).show(5,Truncate=False))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import date_format, dayofweek\n",
    "\n",
    "# get day of the week\n",
    "rc.select(col('Date'),dayofweek(col('Date')), date_format(col('Date'),'E')).show(5)\n",
    "rc.groupBy(date_format(col('Date'),'E')).count().orderBy('count', ascending=False).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pandas to chart to access it, use .collect()\n",
    "rc.groupBy(date_format(col('Date'),'E')).count().collec()\n",
    "dow = [x[0] for x in rc.groupBy(date_format(col('Date')).count().collect())]\n",
    "cnt = [x[1] for x in rc.groupBy(date_format(col('Date')).count().collect())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cp = pd.DataFrame({'Day_of_week':dow, 'Count':cnt})\n",
    "cp.head()\n",
    "cp.sort_values('Count', ascending=False).plot(kind='bar', color='olive' x='Day_of_week', y='Count')\n",
    "plt.xlabel('Day of the week')\n",
    "plt.ylabel('number of reported crimes')\n",
    "plt.title('No. of reported crimes per day of the week from 2001')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHEN TO USE RDD\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
